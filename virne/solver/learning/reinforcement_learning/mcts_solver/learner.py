# Learner component for AlphaZero-style training
from __future__ import annotations

import json
import os
import random
from typing import List

import torch
import torch.nn.functional as F
from torch_geometric.data import Batch

from .net import ActorCritic
from .node import State
from .common import deserialize_state, state_to_obs


class AlphaZeroLearner:
    """Learner that trains on trajectories generated by actors."""

    def __init__(self, controller, recorder, counter, logger, config,
                 replay_dir: str = "replay_buffer", device: torch.device | None = None,
                 batch_size: int = 32):
        self.controller = controller
        self.recorder = recorder
        self.counter = counter
        self.logger = logger
        self.config = config

        self.replay_dir = replay_dir
        self.batch_size = batch_size
        self.policy_path = os.path.join(self.replay_dir, "policy_latest.pt")
        os.makedirs(self.replay_dir, exist_ok=True)

        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy = ActorCritic(
            p_net_num_nodes=config.simulation.p_net_setting_num_nodes,
            p_net_feature_dim=config.simulation.p_net_setting_num_node_resource_attrs,
            v_net_feature_dim=config.simulation.v_sim_setting_num_node_resource_attrs,
            p_net_edge_dim=config.simulation.p_net_setting_num_link_resource_attrs,
        ).to(self.device)
        if os.path.exists(self.policy_path):
            self.policy.load_state_dict(torch.load(self.policy_path, map_location=self.device))
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=1e-4)

    # ------------------------------------------------------------------
    def train_steps(self, num_steps: int) -> None:
        self.policy.train()
        for _ in range(num_steps):
            batch = self._load_and_prepare_batch()
            if batch is None:
                print("Replay buffer is not large enough to start training. Skipping step.")
                break

            batch_obs, batch_policies, batch_values = batch

            self.optimizer.zero_grad()
            predicted_logits = self.policy.act(batch_obs)
            predicted_values = self.policy.evaluate(batch_obs).squeeze()

            # For policy loss, convert policy probabilities to target indices for cross-entropy
            if batch_policies.dim() > 1 and batch_policies.size(1) > 1:
                # If we have probability distributions, use KL divergence
                predicted_probs = F.softmax(predicted_logits, dim=-1)
                loss_policy = F.kl_div(predicted_probs.log(), batch_policies, reduction='batchmean')
            else:
                # Legacy format with single outcomes
                loss_policy = F.cross_entropy(predicted_logits, batch_policies.long())
            
            loss_value = F.mse_loss(predicted_values, batch_values)
            total_loss = loss_policy + loss_value

            total_loss.backward()
            self.optimizer.step()
        torch.save(self.policy.state_dict(), self.policy_path)

    # ------------------------------------------------------------------
    def _deserialize_state(self, state_dict: dict) -> State:
        return deserialize_state(state_dict, self.controller, self.recorder, self.counter)

    def _state_to_obs(self, state: State) -> dict:
        return state_to_obs(state, self.policy, self.controller, self.device)

    # ------------------------------------------------------------------
    def _load_and_prepare_batch(self):
        all_files = [f for f in os.listdir(self.replay_dir) if f.endswith('.json')]
        if len(all_files) < self.batch_size:
            return None

        batch_files = random.sample(all_files, self.batch_size)
        obs_list: List[dict] = []
        batch_policies = []
        batch_values = []

        for fname in batch_files:
            with open(os.path.join(self.replay_dir, fname), 'r') as f:
                data = json.load(f)
            
            # Check if it's new format or legacy format
            if 'static_environment' in data:
                # New format: reconstruct full state from static + dynamic components
                trajectory = data.get('trajectory', [])
                if not trajectory:
                    continue
                
                step = random.choice(trajectory)
                static_env = data['static_environment']
                dynamic_state = step['dynamic_state']
                
                # Reconstruct the full network state for this timestep
                state = self._reconstruct_state_from_components(static_env, dynamic_state)
                obs = self._state_to_obs(state)
                obs_list.append(obs)
                
                # Use the policy and value from MCTS
                batch_policies.append(torch.tensor(step['policy'], dtype=torch.float32))
                batch_values.append(torch.tensor(step['value'], dtype=torch.float32))
            else:
                # Legacy format: deserialize full state
                trajectory = data.get('trajectory', [])
                if not trajectory:
                    continue
                
                step = random.choice(trajectory)
                state = self._deserialize_state(step['state'])
                obs = self._state_to_obs(state)
                obs_list.append(obs)
                batch_policies.append(torch.tensor(step['policy'], dtype=torch.float32))
                batch_values.append(torch.tensor(data['outcome'], dtype=torch.float32))

        if not obs_list:
            return None

        if len(obs_list) == 1:
            batch_obs = obs_list[0]
        else:
            p_net_batch = Batch.from_data_list([o['p_net'] for o in obs_list]).to(self.device)
            hist = torch.cat([o['history_features'] for o in obs_list], dim=0).to(self.device)
            enc = torch.cat([o['encoder_outputs'] for o in obs_list], dim=0).to(self.device)
            curr = torch.cat([o['curr_v_node_id'] for o in obs_list], dim=0).to(self.device)
            remain = torch.cat([o['vnfs_remaining'] for o in obs_list], dim=0).to(self.device)
            mask = torch.cat([o['action_mask'] for o in obs_list], dim=0).to(self.device)
            v_x = torch.cat([o['v_net_x'] for o in obs_list], dim=0).to(self.device)
            batch_obs = {
                'p_net': p_net_batch,
                'history_features': hist,
                'encoder_outputs': enc,
                'curr_v_node_id': curr,
                'vnfs_remaining': remain,
                'action_mask': mask,
                'v_net_x': v_x,
            }

        policies = torch.stack(batch_policies).to(self.device)
        values = torch.stack(batch_values).to(self.device)
        return batch_obs, policies, values



    def _reconstruct_state_from_components(self, static_env: dict, dynamic_state: dict) -> State:
        """Reconstruct a full state from static environment and dynamic state components."""
        from virne.network import PhysicalNetwork, VirtualNetwork
        import networkx as nx
        
        # Reconstruct physical network with current resource usage
        p_graph = nx.Graph()
        
        # Add nodes with current resource state
        p_node_cpu_used = dynamic_state.get('p_node_cpu_used', {})
        for node_info in static_env['physical_network']['nodes']:
            node_id = node_info['id']
            max_cpu = node_info['max_cpu']
            used_cpu = int(p_node_cpu_used.get(str(node_id), 0))
            available_cpu = max_cpu - used_cpu
            
            p_graph.add_node(node_id, 
                           cpu=available_cpu,
                           max_cpu=max_cpu)
        
        # Add edges with current resource state
        p_link_bw_used = dynamic_state.get('p_link_bw_used', {})
        for link_info in static_env['physical_network']['links']:
            u, v = link_info['source'], link_info['target']
            max_bw = link_info['max_bw']
            used_bw = int(p_link_bw_used.get(f"{u}-{v}", p_link_bw_used.get(f"{v}-{u}", 0)))
            available_bw = max_bw - used_bw
            
            p_graph.add_edge(u, v,
                           bw=available_bw,
                           max_bw=max_bw)
        
        # Reconstruct virtual network (SFC request) - this is static
        v_graph = nx.Graph()
        for node_info in static_env['sfc_request']['nodes']:
            node_id = node_info['id']
            cpu_demand = node_info['cpu_demand']
            v_graph.add_node(node_id, cpu=cpu_demand)
        
        for link_info in static_env['sfc_request']['links']:
            u, v = link_info['source'], link_info['target']
            bw_demand = link_info['bw_demand']
            v_graph.add_edge(u, v, bw=bw_demand)
        
        # Create network objects
        p_net = PhysicalNetwork(p_graph)
        v_net = VirtualNetwork(v_graph)
        
        # Create state object
        state = State(p_net, v_net, self.controller, self.recorder, self.counter)
        
        return state