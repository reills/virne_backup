# Learner component for AlphaZero-style training
from __future__ import annotations

import json
import os
import random
from typing import List

import networkx as nx
import torch
import torch.nn.functional as F
from torch_geometric.data import Batch

from .net import ActorCritic
from .node import State


class AlphaZeroLearner:
    """Learner that trains on trajectories generated by actors."""

    def __init__(self, controller, recorder, counter, logger, config,
                 replay_dir: str = "replay_buffer", device: torch.device | None = None,
                 batch_size: int = 32):
        self.controller = controller
        self.recorder = recorder
        self.counter = counter
        self.logger = logger
        self.config = config

        self.replay_dir = replay_dir
        self.batch_size = batch_size
        self.policy_path = os.path.join(self.replay_dir, "policy_latest.pt")
        os.makedirs(self.replay_dir, exist_ok=True)

        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy = ActorCritic(
            p_net_num_nodes=config.simulation.p_net_setting_num_nodes,
            p_net_feature_dim=config.simulation.p_net_setting_num_node_resource_attrs,
            v_net_feature_dim=config.simulation.v_sim_setting_num_node_resource_attrs,
            p_net_edge_dim=config.simulation.p_net_setting_num_link_resource_attrs,
        ).to(self.device)
        if os.path.exists(self.policy_path):
            self.policy.load_state_dict(torch.load(self.policy_path, map_location=self.device))
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=1e-4)

        # caches used when reconstructing observations
        self._p_data = None
        self._v_data = None
        self._encoder_outputs = None

    # ------------------------------------------------------------------
    def train_steps(self, num_steps: int) -> None:
        self.policy.train()
        for _ in range(num_steps):
            batch = self._load_and_prepare_batch()
            if batch is None:
                print("Replay buffer is not large enough to start training. Skipping step.")
                break

            batch_obs, batch_policies, batch_outcomes = batch

            self.optimizer.zero_grad()
            predicted_logits = self.policy.act(batch_obs)
            predicted_values = self.policy.evaluate(batch_obs).squeeze()

            loss_policy = F.cross_entropy(predicted_logits, batch_policies)
            loss_value = F.mse_loss(predicted_values, batch_outcomes)
            total_loss = loss_policy + loss_value

            total_loss.backward()
            self.optimizer.step()
        torch.save(self.policy.state_dict(), self.policy_path)

    # ------------------------------------------------------------------
    def _deserialize_state(self, state_dict: dict) -> State:
        from virne.network import PhysicalNetwork, VirtualNetwork

        p_graph = nx.node_link_graph(state_dict["p_net"])
        v_graph = nx.node_link_graph(state_dict["v_net"])
        p_net = PhysicalNetwork(p_graph)
        v_net = VirtualNetwork(v_graph)

        from .actor import AlphaZeroActor  # to reuse state_to_obs
        dummy_actor = AlphaZeroActor(self.controller, self.recorder, self.counter,
                                     self.logger, self.config, replay_dir=self.replay_dir)
        state = State(p_net, v_net, dummy_actor.controller, dummy_actor.recorder, dummy_actor.counter)
        state.selected_p_net_nodes = list(state_dict.get("selected", []))
        state.v_node_id = state_dict.get("v_node_id", -1)
        if state.selected_p_net_nodes:
            state.p_node_id = state.selected_p_net_nodes[-1]
        return state

    def _state_to_obs(self, actor: AlphaZeroActor, state: State) -> dict:
        return actor._state_to_obs(state)

    # ------------------------------------------------------------------
    def _load_and_prepare_batch(self):
        from .actor import AlphaZeroActor
        actor = AlphaZeroActor(self.controller, self.recorder, self.counter,
                               self.logger, self.config, replay_dir=self.replay_dir)
        all_files = [f for f in os.listdir(self.replay_dir) if f.endswith('.json')]
        if len(all_files) < self.batch_size:
            return None

        batch_files = random.sample(all_files, self.batch_size)
        obs_list: List[dict] = []
        batch_policies = []
        batch_outcomes = []

        for fname in batch_files:
            with open(os.path.join(self.replay_dir, fname), 'r') as f:
                data = json.load(f)
            trajectory = data.get('trajectory', [])
            if not trajectory:
                continue
            step = random.choice(trajectory)
            state = self._deserialize_state(step['state'])
            obs = actor._state_to_obs(state)
            obs_list.append(obs)
            batch_policies.append(torch.tensor(step['policy'], dtype=torch.float32))
            batch_outcomes.append(torch.tensor(data['outcome'], dtype=torch.float32))

        if not obs_list:
            return None

        if len(obs_list) == 1:
            batch_obs = obs_list[0]
        else:
            p_net_batch = Batch.from_data_list([o['p_net'] for o in obs_list]).to(self.device)
            hist = torch.cat([o['history_features'] for o in obs_list], dim=0).to(self.device)
            enc = torch.cat([o['encoder_outputs'] for o in obs_list], dim=0).to(self.device)
            curr = torch.cat([o['curr_v_node_id'] for o in obs_list], dim=0).to(self.device)
            remain = torch.cat([o['vnfs_remaining'] for o in obs_list], dim=0).to(self.device)
            mask = torch.cat([o['action_mask'] for o in obs_list], dim=0).to(self.device)
            v_x = torch.cat([o['v_net_x'] for o in obs_list], dim=0).to(self.device)
            batch_obs = {
                'p_net': p_net_batch,
                'history_features': hist,
                'encoder_outputs': enc,
                'curr_v_node_id': curr,
                'vnfs_remaining': remain,
                'action_mask': mask,
                'v_net_x': v_x,
            }

        policies = torch.stack(batch_policies).to(self.device)
        outcomes = torch.stack(batch_outcomes).to(self.device)
        return batch_obs, policies, outcomes


