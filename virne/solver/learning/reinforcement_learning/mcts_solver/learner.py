# Learner component for AlphaZero-style training
from __future__ import annotations

import json
import os
import random
from typing import List

import torch
import torch.nn.functional as F
from torch_geometric.data import Batch
from torch.utils.tensorboard import SummaryWriter

from .net import ActorCritic
from .node import State
from .common import deserialize_state, state_to_obs


class AlphaZeroLearner:
    """Learner that trains on trajectories generated by actors."""

    def __init__(self, controller, recorder, counter, logger, config,
                 replay_dir: str = None, models_dir: str = None, 
                 device: torch.device | None = None, batch_size: int = 32):
        self.controller = controller
        self.recorder = recorder
        self.counter = counter
        self.logger = logger
        self.config = config

        # Use provided directories or fallback to default
        self.replay_dir = replay_dir if replay_dir else "replay_buffer"
        self.batch_size = batch_size
        
        # Policy path should be in models directory, not replay buffer
        if models_dir:
            self.policy_path = os.path.join(models_dir, "policy_latest.pt")
        else:
            self.policy_path = os.path.join(self.replay_dir, "policy_latest.pt")
            
        os.makedirs(os.path.dirname(self.policy_path), exist_ok=True)
        os.makedirs(self.replay_dir, exist_ok=True)

        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy = ActorCritic(
            p_net_num_nodes=config.simulation.p_net_setting_num_nodes,
            p_net_feature_dim=config.simulation.p_net_setting_num_node_resource_attrs,
            v_net_feature_dim=config.simulation.v_sim_setting_num_node_resource_attrs,
            p_net_edge_dim=config.simulation.p_net_setting_num_link_resource_attrs,
        ).to(self.device)
        
        # Load pretrained weights if specified
        model_loaded = False
        alphazero_model_path = getattr(config.training, 'alphazero_model_path', '')
        resume_training = getattr(config.training, 'resume_training', True)
        
        # Priority 1: Specific model path
        if alphazero_model_path and os.path.exists(alphazero_model_path):
            self.policy.load_state_dict(torch.load(alphazero_model_path, map_location=self.device))
            logger.info(f"Loaded AlphaZero model from: {alphazero_model_path}")
            model_loaded = True
        # Priority 2: Resume from latest policy
        elif resume_training and os.path.exists(self.policy_path):
            self.policy.load_state_dict(torch.load(self.policy_path, map_location=self.device))
            logger.info(f"Resumed training from: {self.policy_path}")
            model_loaded = True
        
        if not model_loaded:
            logger.info("Starting training from scratch (no pretrained model loaded)")
            
        learning_rate = getattr(config.training, 'policy_learning_rate', 1e-4)
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)
        
        # TensorBoard logging
        from virne.utils.config import get_run_id_dir
        tb_log_dir = os.path.join(get_run_id_dir(config), "logs")
        self.writer = SummaryWriter(tb_log_dir)
        self.global_step = 0

    # ------------------------------------------------------------------
    def train_steps(self, num_steps: int) -> None:
        self.policy.train()
        save_interval = getattr(self.config.training, 'save_interval', 10)
        log_interval = getattr(self.config.training, 'log_interval', 1)
        
        epoch_losses = {'policy': [], 'value': [], 'total': []}
        
        for step in range(num_steps):
            batch = self._load_and_prepare_batch()
            if batch is None:
                self.logger.warning("Replay buffer is not large enough to start training. Skipping step.")
                break

            batch_obs, batch_policies, batch_values = batch

            self.optimizer.zero_grad()
            predicted_logits = self.policy.act(batch_obs)
            predicted_values = self.policy.evaluate(batch_obs).squeeze()

            # For policy loss, convert policy probabilities to target indices for cross-entropy
            if batch_policies.dim() > 1 and batch_policies.size(1) > 1:
                # If we have probability distributions, use KL divergence
                predicted_probs = F.softmax(predicted_logits, dim=-1)
                loss_policy = F.kl_div(predicted_probs.log(), batch_policies, reduction='batchmean')
            else:
                # Legacy format with single outcomes
                loss_policy = F.cross_entropy(predicted_logits, batch_policies.long())
            
            loss_value = F.mse_loss(predicted_values, batch_values)
            total_loss = loss_policy + loss_value

            total_loss.backward()
            
            # Calculate gradient norm for monitoring
            grad_norm = torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            self.global_step += 1
            
            # Store losses for epoch summary
            epoch_losses['policy'].append(loss_policy.item())
            epoch_losses['value'].append(loss_value.item())
            epoch_losses['total'].append(total_loss.item())
            
            # TensorBoard logging
            if self.global_step % log_interval == 0:
                self.writer.add_scalar('Loss/Policy', loss_policy.item(), self.global_step)
                self.writer.add_scalar('Loss/Value', loss_value.item(), self.global_step)
                self.writer.add_scalar('Loss/Total', total_loss.item(), self.global_step)
                self.writer.add_scalar('Training/GradNorm', grad_norm.item(), self.global_step)
                self.writer.add_scalar('Training/LearningRate', self.optimizer.param_groups[0]['lr'], self.global_step)
                
                # Policy accuracy (how often network's top action matches MCTS choice)
                if batch_policies.dim() > 1:
                    pred_actions = predicted_logits.argmax(dim=-1)
                    target_actions = batch_policies.argmax(dim=-1)
                    policy_accuracy = (pred_actions == target_actions).float().mean()
                    self.writer.add_scalar('Accuracy/Policy', policy_accuracy.item(), self.global_step)
            
            # Save model periodically
            if (step + 1) % save_interval == 0:
                self._save_model_atomically()
                avg_policy_loss = sum(epoch_losses['policy'][-save_interval:]) / min(save_interval, len(epoch_losses['policy']))
                avg_value_loss = sum(epoch_losses['value'][-save_interval:]) / min(save_interval, len(epoch_losses['value']))
                self.logger.info(f"Learner Step {self.global_step}: Policy={avg_policy_loss:.4f}, Value={avg_value_loss:.4f}")
        
        # Always save at the end and return summary stats
        self._save_model_atomically()
        
        # Return average losses for epoch summary
        if epoch_losses['policy']:
            return {
                'avg_policy_loss': sum(epoch_losses['policy']) / len(epoch_losses['policy']),
                'avg_value_loss': sum(epoch_losses['value']) / len(epoch_losses['value']),
                'avg_total_loss': sum(epoch_losses['total']) / len(epoch_losses['total']),
                'num_training_steps': len(epoch_losses['policy'])
            }
        return None

    def _save_model_atomically(self):
        """Save model using atomic file replacement to prevent race conditions."""
        import tempfile
        
        # Write to temporary file first
        temp_path = self.policy_path + '.tmp'
        torch.save(self.policy.state_dict(), temp_path)
        
        # Atomic rename (works on most filesystems)
        os.rename(temp_path, self.policy_path)

    # ------------------------------------------------------------------
    def _deserialize_state(self, state_dict: dict) -> State:
        return deserialize_state(state_dict, self.controller, self.recorder, self.counter)

    def _state_to_obs(self, state: State) -> dict:
        return state_to_obs(state, self.policy, self.controller, self.device)

    # ------------------------------------------------------------------
    def _load_and_prepare_batch(self):
        """Load and prepare a batch of training data from replay buffer."""
        all_files = [f for f in os.listdir(self.replay_dir) if f.endswith('.json')]
        if len(all_files) < self.batch_size:
            return None

        batch_files = random.sample(all_files, self.batch_size)
        obs_list: List[dict] = []
        batch_policies = []
        batch_values = []

        for fname in batch_files:
            with open(os.path.join(self.replay_dir, fname), 'r') as f:
                data = json.load(f)
            
            # Process standardized episode format
            trajectory = data.get('trajectory', [])
            if not trajectory:
                continue
            
            # Sample a random timestep from the trajectory
            step = random.choice(trajectory)
            
            # Extract training data (already in correct format from actor)
            obs = step.get('observation')
            target_policy = step.get('policy') 
            target_value = step.get('value')
            
            if obs is None or target_policy is None or target_value is None:
                continue
                
            obs_list.append(obs)
            batch_policies.append(torch.tensor(target_policy, dtype=torch.float32))
            batch_values.append(torch.tensor(target_value, dtype=torch.float32))

        if not obs_list:
            return None

        if len(obs_list) == 1:
            batch_obs = obs_list[0]
        else:
            p_net_batch = Batch.from_data_list([o['p_net'] for o in obs_list]).to(self.device)
            hist = torch.cat([o['history_features'] for o in obs_list], dim=0).to(self.device)
            enc = torch.cat([o['encoder_outputs'] for o in obs_list], dim=0).to(self.device)
            curr = torch.cat([o['curr_v_node_id'] for o in obs_list], dim=0).to(self.device)
            remain = torch.cat([o['vnfs_remaining'] for o in obs_list], dim=0).to(self.device)
            mask = torch.cat([o['action_mask'] for o in obs_list], dim=0).to(self.device)
            v_x = torch.cat([o['v_net_x'] for o in obs_list], dim=0).to(self.device)
            batch_obs = {
                'p_net': p_net_batch,
                'history_features': hist,
                'encoder_outputs': enc,
                'curr_v_node_id': curr,
                'vnfs_remaining': remain,
                'action_mask': mask,
                'v_net_x': v_x,
            }

        policies = torch.stack(batch_policies).to(self.device)
        values = torch.stack(batch_values).to(self.device)
        return batch_obs, policies, values



    # Removed: _reconstruct_state_from_components - unused method